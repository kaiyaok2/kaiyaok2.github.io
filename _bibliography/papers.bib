@article{TRAM,
  title={Advancing Automated In-Isolation Validation in Repository-Level Code Translation},
  author={{<u>Kaiyao Ke</u>} and Ibrahimzada, Ali Reza and Pan, Rangeet and Sinha, Saurabh and Jabbarvand, Reyhaneh},
  journal={arXiv preprint arXiv:2511.21878},
  year={2025},
  abstract={Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.},
  url={https://arxiv.org/abs/2511.21878},
  pdf={https://arxiv.org/pdf/2511.21878},
  selected={true}
}

@inproceedings{NIODebugger,
  author = {{<u>Kaiyao Ke</u>}},
  title = {{NIODebugger: A} Novel Approach to Repair {Non-Idempotent-Outcome} Tests with {LLM-Based} Agent},
  year = {2025},
  isbn = {9798331505691},
  publisher = {IEEE Press},
  url = {https://doi.org/10.1109/ICSE55347.2025.00226},
  doi = {10.1109/ICSE55347.2025.00226},
  abstract = {Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotent-outcome (NIO) flaky tests—tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness.This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63\% to 58.72\%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 large-scale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.},
  booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering (ICSE 2025)},
  note = {Acceptance Rate: 21.30\% (245/1150)},
  pages = {1014–1025},
  numpages = {12},
  location = {Ottawa, Ontario, Canada},
  series = {ICSE '25},
  selected={true},
  slides = {NIODebugger-slides.pdf},
  pdf = {NIODebugger-pdf.pdf},
}

@inproceedings{AlphaTrans,
  author = {Ibrahimzada, Ali Reza and {<u>Kaiyao Ke</u>} and Pawagi, Mrigank and Abid, Muhammad Salman and Pan, Rangeet and Sinha, Saurabh and Jabbarvand, Reyhaneh},
  title = {{AlphaTrans: A} Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation},
  year = {2025},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3729379},
  doi = {10.1145/3729379},
  note = {Acceptance Rate: 22.06\% (135/612)},
  abstract = {Code translation transforms programs from one programming language (PL) to another. One prominent use case is application modernization to enhance maintainability and reliability. Several rule-based transpilers have been designed to automate code translation between different pairs of PLs. However, the rules can become obsolete as the PLs evolve and cannot generalize to other PLs. Recent studies have explored the automation of code translation using Large Language Models (LLMs). One key observation is that such techniques may work well for crafted benchmarks but fail to generalize to the scale and complexity of real-world projects with inter- and intra-class dependencies, custom types, PL-specific features, etc. We propose AlphaTrans, a neuro-symbolic approach to automate repository-level code translation. AlphaTrans translates both source and test code, and employs multiple levels of validation to ensure the translation preserves the functionality of the source program. To break down the problem for LLMs, AlphaTrans leverages program analysis to decompose the program into fragments and translates them in the reverse call order. We leveraged AlphaTrans to translate ten real-world open-source projects consisting of ⟨836, 8575, 2719⟩ (application and test) classes, (application and test) methods, and unit tests. AlphaTrans breaks down these projects into 17874 fragments and translates the entire repository. 96.40\% of the translated fragments are syntactically correct, and AlphaTrans validates the translations’ runtime behavior and functional correctness for 27.03\% and 25.14\% of the application method fragments. On average, integrated translation and validation takes 34 hours (min=3, max=121) to translate a project, showing its scalability in practice. For the syntactically or semantically incorrect translations, AlphaTrans generates a report including existing translation, stack trace, test errors, or assertion failures. We provided these artifacts to two developers to fix the translation bugs in four projects. They fixed the issues in 20.1 hours on average (5.5 hours for the smallest and 34 hours for the largest project) and achieved all passing tests. Without AlphaTrans, translating and validating such big projects could take weeks, if not months.},
  booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering (FSE 2025)},
  location = {Trondheim, Norway},
  numpages = {23},
  keywords = {Neuro-Symbolic Code Translation and Validation},
  selected = {true},
  slides = {alphatrans_slides.pdf},
  pdf = {AlphaTrans.pdf},
}

@inproceedings{pytest-ranking,
  author = {Cheng, Runxiang and {<u>Kaiyao Ke</u>} and Marinov, Darko},
  title = {{pytest-ranking}: A Regression Test Prioritization Tool for {Python}},
  year = {2025},
  isbn = {9798400712760},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3696630.3728587},
  doi = {10.1145/3696630.3728587},
  note = {Acceptance Rate: 64.06\% (41/64)},
  abstract = {Regression Test Prioritization (RTP) can find test failures quicker and provide faster feedback to developers to help in debugging. While RTP has been researched for almost three decades, with many research techniques proposed, practical tools and evaluations are sporadic. We present pytest-ranking, a robust tool for Python and its most popular testing framework Pytest. We evaluate our tool on 4,308 builds for 14 open-source Python projects running on the GitHub Actions CI. Our experiments show that our tool integrates well with the Pytest ecosystem, has a low runtime overhead, and finds test failures faster than the default and random order baselines.},
  booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering, Demo Track (FSE Demo 2025)},
  pages = {1089–1093},
  numpages = {5},
  keywords = {software testing, regression testing, test prioritization, Python},
  location = {Trondheim, Norway},
  series = {FSE Companion '25},
  selected = {true},
  slides = {https://www.youtube.com/watch?v=SrnkgTs3uok},
  pdf = {pytest-ranking.pdf},
}

@inproceedings{evaluating-nondex,
  author = {{<u>Kaiyao Ke</u>} and Marinov, Darko},
  booktitle = {Proceedings of the 2nd IEEE/ACM International Flaky Tests Workshop (FTW 2025)},
  title = {{Evaluating NonDex for Modern Java Ecosystem}},
  year = {2025},
  pages = {23-30},
  abstract = {NonDex is a testing approach designed to unveil implementation-dependent (ID) flaky tests stemming from in-correctly relying on a deterministic implementation of a Java API with an underdetermined specification, e.g., iterating over elements of a HashSet object. Since the original Nondex work was published in 2016, we have enhanced the tool functionality and expanded its integration with recent Java versions and build tools like Maven and Gradle. This evolution enables Nondex to analyze a broader range of large, open-source Java projects. This paper investigates our updated Nondex on modern Java projects. We identified 734 ID flaky tests in 31 Maven projects and 267 ID flaky tests in 25 Gradle projects. Comparing these findings to prior work, this study highlights an increase for a modern Java project to contain some ID flaky test(s). We also studied the propagation of ID flakiness through project dependencies and fixed a key non-determinism issue in the Gradle build system itself. Our study emphasizes the importance of proactively employing NOndexto detect and fix flaky tests, preventing potential disruptions in ongoing and future projects. We put all our results at https://github.com/NonDexFTW/NonDex-Experiments.},
  keywords = {Java;Conferences;Ecosystems;Testing},
  doi = {10.1109/FTW66604.2025.00011},
  url = {https://doi.ieeecomputersociety.org/10.1109/FTW66604.2025.00011},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  month = {apr},
  selected = {true},
  slides = {evaluating-nondex.pdf},
  pdf = {evaluating-nondex-paper.pdf},
  location = {Ottawa, Ontario, Canada},
}